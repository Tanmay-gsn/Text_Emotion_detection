{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport re\nimport nltk\n\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom datasets import load_dataset\n\n# Loading Dataset\ndataset = load_dataset(\"dair-ai/emotion\", split=\"train\").train_test_split(test_size=0.2, seed=42)\ndf_train = pd.DataFrame(dataset['train'])\ndf_val = pd.DataFrame(dataset['test'])\n\nlemmatizer = WordNetLemmatizer()\n\ndef clean_and_lemmatize(text):\n    text = text.lower()\n    text = re.sub(r\"http\\S+|www\\S+\", '', text)\n    text = re.sub(r'@\\w+|#\\w+', '', text)\n    text = re.sub(r\"[^a-zA-Z\\s]\", '', text)\n    text = re.sub(r\"\\s+\", ' ', text).strip()\n    tokens = word_tokenize(text)\n    lemmatized = [lemmatizer.lemmatize(word) for word in tokens]\n    return ' '.join(lemmatized)\n\ndf_train['clean_text'] = df_train['text'].apply(clean_and_lemmatize)\ndf_val['clean_text'] = df_val['text'].apply(clean_and_lemmatize)\n\n# Label encoding\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ny_train = le.fit_transform(df_train['label'])\ny_val = le.transform(df_val['label'])\n\n# Tokenization + Padding\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\ntokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')\ntokenizer.fit_on_texts(df_train['clean_text'])\n\nX_train = tokenizer.texts_to_sequences(df_train['clean_text'])\nX_val = tokenizer.texts_to_sequences(df_val['clean_text'])\n\nX_train = pad_sequences(X_train, maxlen=100)\nX_val = pad_sequences(X_val, maxlen=100)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}